{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Heating Load Analysis"]}, {"cell_type": "markdown", "metadata": {}, "source": ["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n", "\n", "KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n", "\n", "* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n", "\n", "You will find instructions below about how to define each variable.\n", "\n", "Once you're happy with your code, upload your notebook to KATE to check your feedback."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## About the data\n", "\n", "The following dataset comes from a study about the heating load required to maintain comfortable indoor air conditions at buildings. That study investigated the effect of eight input variables (relative compactness, surface area, wall area, roof area, overall height, orientation, glazing area, glazing area distribution) on the required heating load.\n", "\n", "This study simulated a total of 768 buildings. Those buildings had different surface areas and dimensions, but the same volume and construction materials.\n", "\n", "The goal of this assignment is to use machine learning techniques for the prediction of the required heating load. Since the output variable of this dataset (required heating load) is a continuous variable, we will treat this problem as \n", "\n", "## About the assignment\n", "\n", "A typical supervised learning task involves creating training and testing sets, performing hyperparameter tuning on the selected algorithm and validating its performance. \n", "\n", "A preliminary study was applied to handle this problem. The selected algorithm was *K-Nearest Neighbours* (KNN). The performance of the method was validated on the testing data by capturing two different metrics: *Mean Absolute Error* (MAE) and *Mean Square Error* (MSE). The findings of this study were:\n", "\n", "**MAE = 1.875**\\\n", "**MSE = 8.495**\n", "\n", "Let\u2019s see if we can do better than that! For this assignment, we will go through the data first and check if we can apply feature engineering. Feature engineering is an important step as it can be used to create or remove features, reduce data complexity and generally understand the data better in order to improve performance. \n", "\n", "We will go through the steps of applying feature engineering, splitting the data into training/testing sets and using *k-fold* cross-validation to tune the parameters of KNN. The performance of the model will be validated using the **MAE** and **MSE** metrics."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setup\n", "\n", "First, let's load all the necessary libraries needed for this assignment.\n", "\n", "We will import `numpy` and `pandas` for all the data manipulation tasks. `matplotlib` and `seaborn` will be used to generate plots and graphs that will assist us during feature engineering. `scipy stats` is useful since it contains a large number of statistical functions.\n", "\n", "Finally, `sklearn` will be used for training the regression model. For this assignment we will need modules about data preprocessing (`MinMaxScaler, OneHotEncoder`), cross-validation (`train_test_split`,`GridSearchCV`,`KFold`), metrics (`mean_absolute_error`,`mean_squared_error`) and the *KNN* regression (`KNeighborsRegressor`)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from scipy import stats as stats \n", "\n", "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n", "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n", "from sklearn.metrics import mean_absolute_error,mean_squared_error\n", "from sklearn.neighbors import KNeighborsRegressor"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Data\n", "\n", "In this assignment we will load two datasets. The first one contains the data needed for training our model (let's save it to a variable called `df`). The second one contains the data needed for evaluating our model with **KATE**. This will be our held-out test data (let's save it to a variable called `df_eval`).\n", "\n", "We will need to process `df_eval` in exactly the same way as `df`, train our model on `df` and make predictions using `df_eval`.\n", "\n", "Run the following cell to load the data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('data/heating_load.csv')\n", "df_eval = pd.read_csv('data/heating_load_eval.csv')\n", "\n", "print(df.shape)\n", "print(df_eval.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_eval.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice that while both `df` and `df_eval` have $9$ columns, the `heating_load` column in `df_eval` is full on null values.\n", "\n", "This is because `heating_load` is our target variable and `df_eval` is our test set. In this assignment, we will use the input features from `df_eval` to predict the `heating_load`, and **KATE** will evaluate these predictions against the actual values. This is what's known as a held-out test set."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Prepare input and output**\n", "\n", "Prepare the input and output variables for this assignment. Store the resulting DataFrames to the following variables:\n", "\n", "* `X_train`: the input features of `df`\n", "* `y_train`: the output variable of `df`\n", "* `X_eval` : the input features of `df_eval`\n", "\n", "*Hint: for `X_eval` just drop the column which contains nothing but null values*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n", "# X_train = ...\n", "# y_train = ...\n", "# X_eval = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Feature engineering\n", "\n", "**1. Analysis for feature engineering**\n", "\n", "In this part, we will investigate how we can manipulate the dataset in order to create/remove/manipulate features and extract useful information from the data. Your task for this part of the assignment is to do just that! \n", "\n", "Manipulate the data and come up with a new representation with the goal of improving performance. You can follow the hints that have been provided in this notebook as a guide.\n", "\n", "It is important to note that following all of the hints **does not** guarantee improved model performance. The hints are some standard methodologies that can be used for feature engineering. \n", "\n", "**It is up to you to implement the analysis you want in order to manipulate the datasets and improve the results at the end.**\n", "\n", "At the end of your analysis, replace the original DataFrames `X_train` and `X_eval`. Please ensure that these variables both are `pd.DataFrame()` Make sure you keep the same variable names, as it is important for the next steps of this assignment.\n", "\n", "Put all your processing within a function (e.g. `feature_engineering()`) instead of plain Python code."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Hint No. 1 \n", "\n", "It is always useful to look at some basic information about the data we are dealing with. Since we are using `pandas`, we can apply the existing methods of the DataFrame `X_train` to check basic information about our dataset. Use the `.info()` and `.describe()` methods."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Hint No. 2 \n", "\n", "It is helpful to create histograms of the examined variables. You can use `matplotlib` and `seaborn` to create those histograms and look at the distribution of the data. Depending on the distribution, we can apply various transformations."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Hint No .3 \n", "\n", "Looking at the correlation values between the features, is a good way of determining which variables could be omitted. You can calculate and visualise the correlation matrix and decide which variables (if any) could be removed/replaced. \n", "\n", "The correlation matrix can be easily computed using the `.corr()` method of the `X_train` DataFrame.\n", "\n", "Visualising the matrix can be achieved with the `seaborn` library."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Hint No. 4 \n", "\n", "It is important to look at the data and discover outliers (if any). Outliers can cause a model to underperform since they differ from the majority of the data. \n", "\n", "Visualising the data is a good method to detect outliers. However, there are some statistical methodologies that perform well and are quick to compute. Using standard deviation and the *z-score* is one such method.\n", "\n", "The *z-score* tells us how many standard deviations away a value is from the mean. If a sample is a certain number of standard deviations away from the mean (e.g. 2-4), then it can be assumed to be an outlier.\n", "\n", "Experiment with the data and determine whether some values could be treated as outliers."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Hint No. 5 \n", "\n", "Some features in the dataset do not have continuous values. Instead they have integer values. In many cases, when a feature has only integer values, then it is possible that those values do not have a numerical meaning. \n", "\n", "Consider an example where there is a feature that describes the seasons ('autumn','winter','spring','summer') by using numbers 1,2,3,4. Even though 2>1 from a numerical standpoint, it makes no sense to say that 'winter'>'autumn'. This trick is useful when we do not want to work with categorical values.\n", "\n", "To handle situations similar to that one, we can use One-Hot-Encoding. This is a trick where we create new binary variables for our dataset. Each new variable represents one of the original options. \n", "\n", "Use the `sklearn` library to perform One-Hot-Encoding for the variable called `orientation`. Use `OneHotEncoder` from `sklearn` and set the parameter `handle_unknown='ignore'`. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Hint No. 6\n", "\n", "It is standard procedure to perform feature scaling before training a model. This way, we can bring all the features to the same range and avoid having some features being more dominant than others.\n", "\n", "You can use the `MinMaxScaler` utility to set the data to pre-defined range between a minimum and a maximum number. This range is usually set to [0,1]."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n", "# def feature_engineering(data):\n", "#    ...\n", "# \n", "# \n", "# X_train = pd.DataFrame(feature_engineering(X))\n", "# X_eval = pd.DataFrame(feature_engineering(X_eval))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cross-validation\n", "\n", "The next step is to use cross-validation to tune the hyperparameters of our regressor(s). Each method (e.g Decision Trees, SVM, Logistic Regression etc) has its own set of hyperparameters that require tuning. Fine tuning is essential in order to address possible under- or over-fitting issues. For this assignment, we will use the `KNeighborsRegressor` from `sklearn`.\n", "\n", "**2. K-Fold validation**\n", "\n", "We will use k-fold validation on the training test in order to pick the best set of parameters.\n", "Use the `KFold` utility to define a k-fold object. Use a 5-fold validation approach. Save it to a variable called `cv_object`. For reproducability purposes, set `shuffle=True` and `random_state=50`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n", "# cv_object = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**3. Hyperparameter tuning**\n", "\n", "Define the values of the grid that is to be explored. To do this, create a variable called `grid_values`. This should be a dictionary where the keys are the names of the hyperparameters of the `KNeighborsRegressor` and the values are lists that contain the desired hyperparameter values.\n", "\n", "For this assignment, create a grid for the following hyperparameters:\n", "\n", "`n_neighbors`, `weights`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n", "# grid_values = {...}\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Having defined the values of the grid, it is now time to use `GridSearchCV`.\n", "\n", "Define a grid search estimator and assign it to a variable called `grid_estimator`. Use `KNeighborsRegressor` as a base estimator. \n", "\n", "There is no need to define any hyperparameters, since we have already done that in the previous step (`grid_values`).\n", "\n", "Use the `cv_object` from the previous step, and `neg_mean_absolute_error` as a scoring metric."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n", "# grid_estimator = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training\n", "\n", "**4. Training phase and identification of best hyperparameters**\n", "\n", "Everything is in place. We can now train our model using the training set. Use the `.fit()` method of the estimator you defined in the previous step. The result will be the best estimator based on the hyperparameter values we defined."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once training is complete, uncomment and run the cell below to view the best parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# grid_estimator.best_params_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have trained our model, let's see how well it performs on the training data.\n", "\n", "Create a variable `y_pred` and store the predictions of the model for the training set:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n", "# y_pred = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculate the performance of our model on the training set using the `mean_absolute_error` and `mean_squared_error` metrics. Assign the results to variables variable called `mae` and `mse` respectively. They should be float numbers, rounded to 3 digits (e.g 1.452)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n", "# mae = ...\n", "# mse = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once complete, uncomment and run the cell below to print the metrics on testing."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# print(\"Mean Absolute Error metric for KNN: {}\".format(mae))\n", "# print(\"Mean Square Error metric for KNN: {}\".format(mse))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Validation\n", "\n", "**5. Validation step on the testing set**\n", "\n", "We can now use this model to make predictions on new, previously unseen data. It is now time to use the testing set and validate the performance of our classifier by submitting to **KATE**.\n", "\n", "Create a variable `y_eval_pred` and store the predictions of the model for the evaluation set. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n", "# y_pred = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["At this point, we have processed our features and trained a model. We have also generated predictions for data without labels (`y_eval_pred`). To see how well our model has performed on the test set, you will have to submit it to **KATE**!"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.9"}}, "nbformat": 4, "nbformat_minor": 4}