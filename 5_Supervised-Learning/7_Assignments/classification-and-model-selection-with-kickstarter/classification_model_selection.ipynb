{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Classification and Model Selection\n", "\n", "## Classifying Kickstarter Campaigns\n", "\n", "Kickstarter is a crowdfunding platform with a community of more than 10 million people comprising of creative, tech enthusiasts who help in bringing new projects to life.\n", "\n", "Until now, more than $3 billion dollars have been contributed by the members in fueling creative projects.\n", "The projects can be literally anything \u2013 a device, a game, an app, a film, etc.\n", "\n", "Kickstarter works on all or nothing basis: a campaign is launched with a certain amount they want to raise, if it doesn't meet its goal, the project owner gets nothing. For example: if a projects's goal is $\\$5000$ and it receives $\\$4999$ in funding, the project won't be a success.\n", "\n", "If you have a project that you would like to post on Kickstarter now, can you predict whether it will be successfully funded or not? Looking into the dataset, what useful information can you extract from it, which variables are informative for your prediction and can you interpret the model?\n", "\n", "The goal of this project is to build a classifier to predict whether a project will be successfully funded or not. \n", "\n", "**\ud83d\udca1 You can use any algorithm of your choice.**\n", "\n", "We will use `sklearn` and the usual data science libraries such as `pandas` and `numpy`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n", "\n", "KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n", "\n", "* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n", "\n", "You will find instructions below about how to define each variable.\n", "\n", "Once you're happy with your code, upload your notebook to KATE to check your feedback."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Baseline Model\n", "\n", "In this exercise, we are looking to outperform the performance of a simple `baseline` model. This `baseline` is a simple logistic regression with only two features: `goal_usd` (adjusted goal) and `usa` (whether the campaign happened in the US)\n", "\n", "The code to build this `baseline` is shown below:\n", "\n", "```Python\n", "from sklearn.linear_model import LogisticRegression\n", "\n", "# Conduct some custom processing on your training data\n", "df[\"usa\"] = df[\"country\"] == \"US\"\n", "df[\"goal_usd\"] = df[\"goal\"] * df[\"static_usd_rate\"]\n", "\n", "df = df[[\"goal_usd\", \"usa\", \"state\"]]\n", "\n", "# Conduct the same processing on your testing data\n", "df_eval[\"usa\"] = df_eval[\"country\"] == \"US\"\n", "df_eval[\"goal_usd\"] = df_eval[\"goal\"] * df_eval[\"static_usd_rate\"]\n", "\n", "df_eval = df_eval[[\"goal_usd\", \"usa\", \"state\"]]\n", "\n", "X = df.drop([\"state\"], axis=1)\n", "y = df[\"state\"]\n", "\n", "X_eval = df_eval.drop([\"state\"], axis=1)\n", "\n", "model = LogisticRegression()\n", "model.fit(X, y)\n", "\n", "y_pred = model.predict(X_eval)\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Our Model\n", "\n", "To kick things off, let's import and use our favourite data processing library, `pandas`, to retrieve the data that we will use to build a machine learning model.\n", "\n", "In this assignment, we are going to load in two datasets. The first, `df`, is going to contain all the data we will need to train and test a model. This will include the labels indicating whether or not the project was successfully funded. The second dataset, `df_eval`, is going to contain all the data that our model will be evaluated on by **KATE**. It does not include the labels indicating project success, so can be viewed as held-out test data. \n", "\n", "We will need to process `df_eval` in exactly the same way as `df`, then use our model trained on `df` to make predictions about `df_eval`. On submission, **KATE** will evaluate these predictions against their labels (which **KATE** has access to).\n", "\n", "\n", "Run the cell below to load the raw data. Note that `pandas` is pretty smart and can read these ZipFiles into regular `DataFrames`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "df = pd.read_csv(\"data/kickstarter.gz\")\n", "df_eval = pd.read_csv(\"data/kickstarter_eval.gz\")\n", "\n", "print(df.shape)\n", "print(df_eval.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have also displayed the dimensions of our `df` and `df_eval`. Notice that the `df_eval` is only $10,000$ rows. As mentioned earlier, this will be our test set for submissions to **KATE**.\n", "\n", "**This means that we cannot train on `df_eval`**\n", "\n", "\n", "The aim of this practical is to:\n", "  * Process `df` into an input dataframe `X` and a label dataframe `y`\n", "  * Process `df_eval` into an input dataframe `X_eval` (in the same way as we processed `df` into `X`)\n", "  * Train a classification model of our choice on `X` and `y`\n", "  * Submit our code to KATE, where our model will be evaluated on `X_eval` and `y_eval`\n", "\n", "Let's kick things off by checking out our `df`. Note that the `state` column contains our success labels:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In addition to using the `.head()` function, let's also retrieve some more information about our data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.info()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["From the above, we can see that there are $50,000$ projects in `df` and, apart from a handful of columns, most of our data is not null - what a relief!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_eval.info()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Unlike `df`, `df_eval` contains only $10,000$ projects. Also note that the target variable, `state` is null for all entries - as mentioned earlier, this is stored on **KATE** for evaluating our model when we submit our code."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"df labels:      \", df.state.unique())\n", "print(\"df_eval labels: \", df_eval.state.unique())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Notes on the dataset**:\n", "* The target `state` corresponds to a binary outcome: `0` for failed, `1` for successful. \n", "* The variables `'deadline'`, `'created_at'`, `'launched_at'` are stored in Unix time format."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: Preprocessing the data\n", "\n", "Although our data is relatively clean, it is not yet in a state where we can train a model. For instance, both `df` and `df_eval` contains columns used for training (features) as well as the target column although this is, of course, null for `df_eval`.\n", "\n", "What we have to do now is preprocess our data. Specifically, we need to:\n", " - Build a training set: `X` and `y`\n", " - Build an evaluation set: `X_eval`\n", " \n", " <br>\n", " \n", "Let's start by extracting the `state` column from `df` into a variable called `y`:\n", " - Create a new variable `y` from `df[\"state\"]`\n", " - Drop the `state` column from `df` and `df_eval`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = df[\"state\"]\n", "\n", "df.drop(\"state\", axis=1, inplace=True)\n", "df_eval.drop(\"state\", axis=1, inplace=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.1 Remove redundant columns\n", "\n", "After removing the target variable `y` from our input data, we can start processing.\n", "\n", "Remove all the columns that **you** think are not salient for this classification task. For instance, `id`, `photo`, `slug`, and `disable_communication` are some features which are not likely to be relevant. The choice of which features to retain, however, is yours to make. Remember to remove the same columns from `df` as `df_eval`.\n", "\n", "You can use the `.drop()` function from `pandas` to remove columns (remember to specify `axis=1`)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here:\n", "# columns_to_drop = [...]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.2 Fill null values\n", "\n", "Looking at the output of `df.info()` above, we can see that some of the columns which we might be interested in as features contain some null values. Null values are, in general, a problem for machine learning models and can cause your code to break. How you choose to deal with them, however, will depend in large part on how you intend to process your data. For instance, if your input data consists of strings that you wish to generate a word count feature from, you can just fill in the null values with empty strings (`\"\"`).\n", "\n", "Thankfully, `pandas` has a helpful function for dealing with null values: the `.fillna()` function. Remember to do the same to `df` as `df_eval`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here:\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.3 Additional Processing\n", "\n", "In the previous two exercises we have covered the most basic steps in preprocessing: dropping redundant columns and working with null values. However, there is *so* much more that we can do to extract useful information from our data. \n", "\n", "For instance, the `blurb` column contains unique strings and so, in its current form, isn't a particularly useful feature. Instead, we could create a new feature representing the length of the `blurb`, or the number of words in the `blurb`. \n", "\n", "Other string-type columns, such as `country`, contain categorical data. As there are a lot of countries represented, we might want to aggregate these into regions (e.g. `Europe`, `Asia`, ...). We can then convert this categorical data into a one-hot encoding using `sklearn`.\n", "\n", "What we are describing here is what's known as feature engineering and is an art and a science in its own right. \n", "\n", "Let's start this processing by importing some libraries and functions that can help us create features. Notice that we import the `StandardScaler` from `sklearn`. We can use this function on our numerical data to normalise it, which is an important step in training a machine learning model.\n", "\n", "**\ud83d\udca1 In the following cell, you can use feature engineering to create features that you think might be useful.**\n", "\n", "<br>\n", "\n", "You may want to put all your processing within a function (such as `processing()`) or may want to do it just as plain Python code. It's entirely up to you!\n", "\n", "However, once you have processed `df` and `df_eval`, you must assign them to input variables `X` and `X_eval`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import json\n", "import numpy as np\n", "\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n", "\n", "# Your code here:\n", "#\n", "# def processing(df):\n", "#    ...\n", "#\n", "# X = processing(df)\n", "# X_eval = processing(df_eval)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Training the model\n", "\n", "Now that we have separated our data into train and evaluation data, we can start training models and evaluating their performance. At this point, you are welcome to explore any model architecture, so long as it is a **classification** model.\n", "\n", "Check out the `sklearn` [documentation](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) for a selection of possible models and implementation examples.\n", "\n", "Note that most `sklearn` models have the same interface. Once imported you can create an instantiation of your model (specifying custom settings as you see fit), and assign it to a variable. For instance:\n", "\n", "``` Python\n", "from sklearn.linear_model import LogisticRegression\n", "\n", "model = LogisticRegression()\n", "```\n", "\n", "Once you have created your `model` variable, you can call `.fit()` and pass `X` and `y` as arguments.\n", "\n", "**\ud83d\udca1 For KATE to work, your model must be assigned to a variable called `model`**\n", "\n", "**NOTE**: Since with this project your model will be trained directly on KATE, it is limited to models that can be trained under 1min. You will receive a `TimeoutError` if your model takes too long.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here:\n", "# model = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once trained, we can use the `.score()` function to evaluate our model's performance on the train set. Remember to pass `X` and `y` as arguments."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here:\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Making predictions\n", "\n", "Now that our model is trained, we can use the `.predict()` function to make predictions for the rows in our data where `y` is not known.\n", " - Call `.predict()` on the `model` variable, and pass `X_eval`\n", " - Assign the output of `.predict()` to a variable called `y_pred`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here:\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that in the previous exercise, we used the `.score()` function to evaluate our model on the training data. However, we do not have the ground truth for our `X_eval` data points - to see how well the model performs on the test set, you will have to submit it to **KATE**!"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.12"}}, "nbformat": 4, "nbformat_minor": 4}